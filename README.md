# ğŸ“Š RAG Complaint Intelligence System

A Retrieval-Augmented Generation (RAG) system for analyzing financial customer complaints using semantic search and LLMs.

---

## ğŸ“‹ Project Overview

Financial institutions receive thousands of customer complaints every year, making it difficult for analysts and decision-makers to quickly understand recurring issues, customer pain points, and company-level trends. Traditional keyword search and manual analysis are slow, inconsistent, and not scalable.

This project implements a **Retrieval-Augmented Generation (RAG)** system that enables users to ask natural language questions about financial complaints and receive context-aware, evidence-backed answers. The system combines semantic search with a Large Language Model (LLM) to deliver accurate, explainable insights grounded in real complaint data.

---

## ğŸ¯ Solution

We built a modular RAG pipeline that:

- ğŸ” **Retrieves** the most relevant complaint text chunks using vector similarity search (FAISS)
- ğŸ“ **Injects** retrieved context into a carefully designed prompt
- ğŸ¤– **Generates** grounded answers using a transformer-based LLM
- ğŸ“ **Returns** both the answer and the source evidence, increasing transparency and trust

This solution is designed for **financial analysts, compliance teams, and product managers** who need fast, reliable insights from large volumes of unstructured complaint data.

---

## â­ Key Features

| Feature | Description |
|---------|-------------|
| ğŸ” **Semantic Search** | Search over thousands of complaint narratives using vector embeddings |
| ğŸ§  **RAG Pipeline** | Retrieval-Augmented Generation for accurate, contextual answers |
| âœ… **Evidence-Backed Answers** | Responses include source text for verification |
| ğŸ§© **Modular Architecture** | Clean, testable Python code with separation of concerns |
| ğŸ§ª **Unit Testing** | Comprehensive tests with pytest |
| ğŸ”„ **CI/CD** | Automated testing with GitHub Actions |
| ğŸ“¦ **Reproducible** | Production-oriented design with configuration management |

---

## ğŸ—ï¸ Architecture
```bash

                  User Question
                     â”‚
                     â–¼
          Query Embedding
     (Sentence Transformers)
                     â”‚
                     â–¼
          FAISS Vector Store
            (Similarity Search)
                     â”‚
                     â–¼
       Top-K Relevant Complaint Chunks
                     â”‚
                     â–¼
             Prompt Builder
        (Context + Question)
                     â”‚
                     â–¼
             LLM Generator
                (FLAN-T5)
                     â”‚
                     â–¼
          Final Answer + Sources
```


---

## ğŸ› ï¸ Core Technologies

- **Python 3.10**
- **Pandas, NumPy** â€“ data processing
- **Sentence-Transformers** â€“ embeddings
- **FAISS** â€“ vector similarity search
- **Hugging Face Transformers** â€“ LLM
- **Gradio** â€“ web UI
- **pytest** â€“ unit testing
- **GitHub Actions** â€“ CI/CD
- **Jupyter Notebooks** â€“ experimentation & evaluation

---

## ğŸ“ Project Structure

```bash
rag-complaint-intelligence-capstone/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ complaint_embeddings.parquet
â”‚
â”œâ”€â”€ vector_store/
â”‚   â””â”€â”€ faiss/
â”‚       â”œâ”€â”€ index.faiss
â”‚       â””â”€â”€ metadata.pkl
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ evaluation.ipynb      # Jupyter notebook for analysis & evaluation
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py              # Configuration (models, paths, constants)
â”‚   â”œâ”€â”€ vector_store.py        # FAISS loading logic
â”‚   â”œâ”€â”€ retriever.py           # Similarity search
â”‚   â”œâ”€â”€ prompt.py              # Prompt templates
â”‚   â”œâ”€â”€ generator.py           # LLM inference
â”‚   â”œâ”€â”€ pipeline.py            # RAG orchestration
â”‚   â”œâ”€â”€ evaluation.py          # Qualitative evaluation utilities
â”‚   â””â”€â”€ utils.py               # Helper utilities
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_vector_store.py
â”‚   â”œâ”€â”€ test_retriever.py
â”‚   â”œâ”€â”€ test_generator.py
â”‚   â””â”€â”€ test_prompt.py
â”‚   â””â”€â”€ test_pipeline.py
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci.yml             # GitHub Actions CI
â”‚
â”œâ”€â”€ app.py                     # Gradio UI
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md

```



---

## ğŸ“Œ Tasks Overview

### âœ… Task 1: Project Selection & Gap Analysis
**Objective:** Select capstone project and create structured improvement plan.

**Key Activities:**
- Reviewed previous RAG implementation
- Identified gaps in modularity, testing, and CI
- Defined high-impact engineering improvements

**Deliverables:**
- âœ… Gap analysis completed
- âœ… Improvement plan defined
- âœ… Project structure designed

---

### âœ… Task 2: Engineering Excellence
**Objective:** Implement robust engineering improvements.

**Key Components:**
- Code refactoring with type hints and modular design
- Unit testing with pytest (5 tests covering core components)
- GitHub Actions CI pipeline for automated testing
- Production-grade configuration management

**Modules:**
- `src/config.py` â€“ Type-safe configuration
- `src/vector_store.py` â€“ FAISS operations
- [src/retriever.py](cci:7://file:///d:/10acadamyWeek7%20project/rag-complaint-intelligence/src/retriever.py:0:0-0:0) â€“ Semantic search
- `src/prompt.py` â€“ Prompt engineering
- `src/generator.py` â€“ LLM inference
- `src/pipeline.py` â€“ RAG orchestration

**Deliverables:**
- âœ… Modular codebase with clear structure
- âœ… 5 comprehensive unit tests
- âœ… Working CI/CD pipeline
- âœ… Type hints and documentation

---

### âœ… Task 3: RAG Pipeline Implementation & Evaluation
**Objective:** Build and evaluate the full RAG pipeline.

**Key Components:**
- **Retriever:** Semantic similarity search (top-k retrieval)
- **Prompt Engineering:** Finance-focused, context-only answers
- **Generator:** LLM-based response generation
- **Evaluation:** Qualitative assessment with representative queries

**Deliverables:**
- âœ… End-to-end RAG pipeline
- âœ… Evaluation framework
- âœ… Quality analysis and scoring

---

## â–¶ï¸ Installation & Running the App

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/bethywa/rag-complaint-intelligence-capstone.git
cd rag-complaint-intelligence-capstone

2ï¸âƒ£ Create Virtual Environment
python -m venv .venv
.venv\Scripts\activate     # Windows
source .venv/bin/activate  # Linux/Mac

3ï¸âƒ£ Install Dependencies
    pip install -r requirements.txt

4ï¸âƒ£ Run the Application
    gradio run app.py

5ï¸âƒ£ Run Unit Tests
  pytest

6ï¸âƒ£ Explore the evaluation notebook
    notebooks/evaluation.ipynb

```


---

## ğŸ§ª System Evaluation (Qualitative)

The system was evaluated using **representative business questions** relevant to financial complaint analysis.

| Question | Answer Quality | Notes |
|--------|----------------|------|
| Credit card dispute complaints | â­â­â­â­ | Grounded in retrieved evidence |
| Unauthorized charges | â­â­â­â­ | Clear and concise |
| Chargeback issues | â­â­â­ | Some abstraction loss |
| Account closures | â­â­â­ | Needs stronger summarization |

**Key Insight:**  
Increasing `top_k` improves recall but can introduce noise â€” optimal balance is required.

---

## ğŸ–¥ï¸ Interactive Gradio Application

### Features:
- Natural language question input
- AI-generated answers
- Source document display (trust & transparency)
- Clear/reset conversation
- User-friendly layout for non-technical users

### Run the App:
```bash
python app.py

Then open:
 http://127.0.0.1:7860
 
```



### âœ… Engineering Improvements 
- Compared to earlier versions, this capstone includes:
   - âœ… Fully modular Python codebase
   - âœ… Dataclass-based configuration
   - âœ… Unit tests (5 core components)
   - âœ… GitHub Actions CI pipeline
   - âœ… Improved embedding & LLM models
   - âœ… Qualitative evaluation framework
   - âœ… Production-style UI

### ğŸš§ Planned Improvements
- Response streaming in UI
- Dockerized deployment
- REST API endpoint
- Advanced evaluation metrics

### ğŸ“ Important Notes
This project was developed as part of the 10 Academy KAIM Program and demonstrates best practices in:
 - ğŸ“Š Data Engineering - Vector embeddings, FAISS indexing
 - ğŸ§  NLP & LLM Systems - RAG architecture, prompt engineering
 - ğŸ”§ Software Engineering - Modular design, unit testing
 - ğŸ”„ CI/CD - GitHub Actions automation

 ğŸ‘¤ Author
- Bethelihem Weldegebrial
### AI & Data Engineering Trainee
 ### 10 Academy â€“ KAIM Program

